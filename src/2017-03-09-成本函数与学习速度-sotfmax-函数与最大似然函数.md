#  成本函数与学习速度 sotfmax 函数与最大似然函数

本文参考 http://neuralnetworksanddeeplearning.com/chap3.html

softmax 函数可以替代 sigmoid 函数，同样，我们也需要修改成本函数，用最大似然函数替代平法和函数。

softmax 函数

\\[
a _ j = \frac{e^{z _ j}}{\sum _ k e^{z _ {k}}}
\\]

这个函数有啥特点呢？就是每一级神经元的所有输出的和恒为 \\(1\\)

\\[
\sum _ {j} a _ j = 1
\\]

也就是说，类似一家公司，\\(e^{z _ j}\\) 就是每个神经元的出资，很有钱，随着输入 \\(z _ j\\) 成指数增长。
神经元的输出就是，这个神经元在本层中所占的股份多少。

类似书写识别，有的时候，我们更希望看到输出的结果是，有多少的概率可能是 0 , 有多少的概率是 1 ，诸如此类。 这个时候 softmax 函数看起来就很应景。


配合 softmax 函数，对应的成本函数就是最大似然函数

\\[ C = - \ln a _ y \\]

首先， \\(C\\) 恒大于零，因为 \\(a _ y\\) 在 \\(0..1\\) 之间。 \\(C\\) 表示对应输出如果是目标 y 的时候的似然概率。

如果 \\(a _ y\\) 等于 1 ，那么其他神经元的输出，对应 \\(y _ j \neq y\\) 的 \\(a _ {y _ j} = 0\\)， 这是啥意思？

就是说，我们选取的权重和偏移量是多么的完美，导致其他神经元都识别说，是 \\(y _ j\\) 的概率为 \\(0\\) ，只有
对应的 \\(a _ y\\) 说，是 \\(1\\)  。

（TODO: 我自己有一个问题，如果不是识别 0-9 个手写字符，而是物品识别，就是说 \\(y\\) 的空间非常大，这个方法合适吗？)

经过一番推导，学习速度和 cross entropy 类似。

(TODO: 为啥选取这个东西，意义不明显)

(TODO: 问题：softmax 函数的输出依赖于其他神经元，BP 的推导过程也许又不一样的地方)
